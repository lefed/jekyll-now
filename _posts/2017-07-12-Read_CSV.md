---
layout: post
title: Reading CSV Files to a dataframe using Pandas
---

### How to use Pandas to read data into a dataframe

The first step of any data analysis is obtaining data and putting it into a format that can be read and analysed. So let's start with a quick how-to on this simple but important data step.

Pandas makes this easy - simply use the built in [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) functionality.

To give your file the ability to use this functionality, be sure to import the pandas library using the standard convention `import pandas as pd`.

#### Reading a data file into a dataframe from your computer

Let's say you've found a dataset and have stored it on your computer. All you would need to do to create a new dataframe that you can start to analyze would be to `import pandas as pd` and then create a dataframe object using `pd.read_csv` - an object name including `df` would be per convention. If your file is stored in the same location as your script, just the filename is needed vs. the absolute or relative path to the file.

`import pandas as pd`

`df = pd.read_csv("your_filename.csv")`

Now if you were to call `df.head()` to look at your first rows of data, you would see the first rows of your newly created dataframe.

#### Reading a data file into a dataframe from a url

Instead of downloading and storing the data on your computer, you can simply read a file right from a web address. Similar to above, we simply create a dataframe object and read in the information.

`import pandas as pd`

`df = pd.read_csv('url')`

Again, if you were to call `df.head()` to look at your first rows of data, you would see the first rows of your newly created dataframe.

#### Reading a data file into a dataframe from multiple urls

What if we want to take multiple web hosted files and create one dataframe? We could do this with a for loop and concatinating the files. 

The following example looks at three csv files of NYC transit data and combines them into one dataframe.

First we create a list of the urls - in this case we give it the name urls.

`urls = ['http://web.mta.info/developers/data/nyct/turnstile/turnstile_170617.txt', 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_170624.txt', 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_170701.txt']`

Then all we have to do is read each of the urls into a separate dataframe and concatinate the files.

`dfs = [pd.read_csv(url) for url in urls]`

`df = pd.concat(dfs, ignore_index=True)`

The above code could be easily modified to read multiple files from your computer.

### N.B. - Not all files will be comma separated values or have default encoding even though we call read_csv and it defaults 

In order to solve this, simply use the additional parameters of `sep` and `encoding`

Example:

The following reads a UTF-16 encoded file with tab separation.

`df = pd.read_csv('filename', sep = '\t', encoding = 'UTF-16')`
